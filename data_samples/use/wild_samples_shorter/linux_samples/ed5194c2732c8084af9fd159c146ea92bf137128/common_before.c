#define NO_MELTDOWN	BIT(1)
#define NO_SSB		BIT(2)
#define NO_L1TF		BIT(3)

#define VULNWL(_vendor, _family, _model, _whitelist)	\
	{ X86_VENDOR_##_vendor, _family, _model, X86_FEATURE_ANY, _whitelist }

	VULNWL(INTEL,	5, X86_MODEL_ANY,	NO_SPECULATION),
	VULNWL(NSC,	5, X86_MODEL_ANY,	NO_SPECULATION),

	VULNWL_INTEL(ATOM_SALTWELL,		NO_SPECULATION),
	VULNWL_INTEL(ATOM_SALTWELL_TABLET,	NO_SPECULATION),
	VULNWL_INTEL(ATOM_SALTWELL_MID,		NO_SPECULATION),
	VULNWL_INTEL(ATOM_BONNELL,		NO_SPECULATION),
	VULNWL_INTEL(CORE_YONAH,		NO_SSB),

	VULNWL_INTEL(ATOM_AIRMONT_MID,		NO_L1TF),
	VULNWL_INTEL(ATOM_GOLDMONT,		NO_L1TF),
	VULNWL_INTEL(ATOM_GOLDMONT_X,		NO_L1TF),
	VULNWL_INTEL(ATOM_GOLDMONT_PLUS,	NO_L1TF),

	VULNWL_AMD(0x0f,		NO_MELTDOWN | NO_SSB | NO_L1TF),
	VULNWL_AMD(0x10,		NO_MELTDOWN | NO_SSB | NO_L1TF),
	VULNWL_AMD(0x11,		NO_MELTDOWN | NO_SSB | NO_L1TF),
	VULNWL_AMD(0x12,		NO_MELTDOWN | NO_SSB | NO_L1TF),

	/* FAMILY_ANY must be last, otherwise 0x0f - 0x12 matches won't work */
	VULNWL_AMD(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF),
	VULNWL_HYGON(X86_FAMILY_ANY,	NO_MELTDOWN | NO_L1TF),
	{}
};

static bool __init cpu_matches(unsigned long which)
	if (ia32_cap & ARCH_CAP_IBRS_ALL)
		setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);

	if (cpu_matches(NO_MELTDOWN))
		return;

	/* Rogue Data Cache Load? No! */